
\documentclass[11pt]{article}

\usepackage{common}
\title{HW3: Language Modeling}
\author{Jeffrey Ling \\ jling@college.harvard.edu \and Rohil Prasad \\ prasad01@college.harvard.edu }
\begin{document}

\maketitle{}
\section{Introduction}

In this assignment, we examine the problem of language modeling. Given a set of words, we want to determine which word comes after it. Note, however, that there may not be a ``best'' word to follow a prefix. For example, ``fish'' and ``chicken'' could both be almost equally likely to follow the prefix ``I ate \_\_\_''. Therefore, it makes more sense to try and find a good \textbf{probability distribution} of the next word given the prefix. We will define what it means to be ``good'' later in the paper. 

We implement and discuss several different methods of language modeling on the Penn Treebank in this paper. Our first class of algorithms are probabilistic count-based algorithms, consisting of maximum likelihood estimation, Laplace smoothing, Witten-Bell smoothing, and Kneser-Ney smoothing. Our second class of algorithms are deep-learning algorithms, consisting of the neural network described in [Bengio]. Furthermore, we also experiment with augmenting this neural network with word embeddings learned via Noise Contrastive Estimation (NCE). 

In Section 2, we give a formal description of our problem and establish our notation. In Section 3, we give detailed descriptions of the algorithms listed above. In Section 4, we present our experimental results. In Section 5, we discuss our findings. 

\section{Problem Description}

Assume our training corpus consists of a sequence of words $w_1, w_2, \dots, w_N$ where $w_i$ is drawn from a vocabulary $\mathcal{V}$ for every $i$. 

Our training data represents this corpus as a set of tuples $(\mathbf{x}_i, y_i)$, where $\mathbf{x}_i$ is the prefix $w_1w_2\dots w_{i-1}$ and $y_i$ is equal to the suffix $w_i$. 

Our goal is to train a model that will take in an input sequence $\mathbf{x}$ of words and output a vector $\mathbf{y} \in \mathbb{R}^{1 \times |\mathcal{V}|}$ where $\mathbf{y}_i = p(v_i | x)$ for $v_i$ the $i$th element of $|\mathcal{V}|$.

\subsection{Count-Based Model Notation}

We use some specific notation for our count-based models. 
\begin{itemize}
  \item $w^{i-1}_{i-n+1}$ is our shorthand for the size $n-1$ prefix $w_{i-n+1}w_{i-n+2}\dots w_{i-1}$ of $w_i$. When $n = 2$, we will continue to use $w_{i-1}$. 
  \item $c(w_i)$ denotes the total number of occurrences of the word $w_i$ in the training corpus. 
  \item $c(w^{i-1}_{i-n+1}, w_i)$ denotes the total number of occurrences of the word $w_i$ after the prefix $w^{i-1}_{i-n+1}$ in the training corpus. 
  \item $N(\cdot, w_i)$ denotes the total number of unique prefixes of size $n$ that $w_i$ has in the training corpus. 
  \item $N(w^{i-1}_{i-n+1}, \cdot)$ denotes the total number of unique suffixes that the sequence $w^{i-1}_{i-n+1}$ has in the training corpus. 
\end{itemize}

\subsection{Evaluation}

Unlike earlier assignments, we are outputting a \emph{distribution} of possibilities given an input $\mathbf{x}$ rather than a \emph{prediction} of the word following $\mathbf{x}$. 

Therefore, we will evaluate by a metric called \emph{perplexity}. 

\section{Model and Algorithms}

\subsection{Count-Based Models}

All of our count-based models are $n$-gram models, meaning we fix an $n$ and assume $p(w_i|w^{i-1}_1) = p(w_i|w^{i-1}_{i-n+1})$. 

\subsubsection{Maximum Likelihood Estimation}

The maximum likelihood estimation $p_{MLE}(w_i|w^{i-1}_{i-n+1})$ is given by 
$$p_{MLE}(w_i|w^{i-1}_{i-n+1}) = \frac{c(w^{i-1}_{i-n+1}, w_i)}{\sum_{w' \in \mathcal{V}} c(w^{i-1}_{i-n+1}, w')}$$

This is by definition the probability over the training corpus of the word $w_i$ appearing conditioned on the prefix being equal to $w^{i-1}_{i-n+1}$. 

\subsubsection{Laplace Smoothing}

 One thing to note is that our MLE model will by definition assign a (clearly erroneous) probability of $0$ to $p(w|w^{i-1}_{i-n+1})$ for any $w^{i-1}_{i-n+1}$ that does not appear in the training corpus. This poses a problem for larger $n$, since the number of distinct $n$-grams in the training corpus will only form a small fraction of the number $|\mathcal{V}|^n$ of possible $n$-grams.

The traditional maximum likelihood $n$-gram model by definition will often erroneously assign a probability of $0$ to $p(w|w^{i-1}_{i-n+1})$. 

We fix this by adding $\alpha$ to the count of every word. Then, our probability becomes 
$$p_L(w_i|w^{i-1}_{i-n+1}, \alpha) = \frac{\alpha + c(w^{i-1}_{i-n+1}, w_i)}{\alpha \cdot |\mathcal{V}| + \sum_{w' \in \mathcal{V}} c(w^{i-1}_{i-n+1}, w')}$$

\subsubsection{Witten-Bell Smoothing}

Witten-Bell smoothing is a form of interpolative smoothing. Interpolative smoothing attempts to handle the problem of MLE a little differently. To prevent a prediction of $0$ for a prefix unseen in the training corpus, 

$$1 - \lambda_{w_{i-1}} = \frac{N(w_{i-1}, \cdot)}{N(w_{i-1}, \cdot) + c(w_{i-1}, w_i)}$$

$$p_{WB}(w_i|w_{i-1}) = \lambda_{w_{i-1}} p_L(w_i|w_{i-1}) + (1-\lambda_{w_{i-1}}) p_L(w_i)$$

$$1 - \lambda_{w^{i-1}_{i-n+1}} = \frac{N(w^{i-1}_{i-n+1}, \cdot)}{N(w^{i-1}_{i-n+1}, \cdot) + c(w_i, w^{i-1}_{i-n+1})}$$ 

$$p_{WB}(w_i|w^{i-1}_{i-n+1}) = \lambda_{w^{i-1}_{i-n+1}} p_L(w_i|w^{i-1}_{i-n+1}) + (1-\lambda_{w^{i-1}_{i-n+1}}) p_{WB}(w_i|w^{i-1}_{i-n+2})$$

\subsubsection{Kneser-Ney Smoothing}

Kneser-Ney smoothing is an example of the ``absolute discounting'' paradigm of smoothing. 

$$p_{KN}(w_i) = N(\cdot, w_i)/\sum_{w' \in \mathcal{V}} N(\cdot, w')$$

$$p_{KN}(w_i|w^{i-1}_{i-n+1}) = \frac{\text{max}\{c(w_i,w^{i-1}_{i-n+1})-\delta, 0\}}{\sum_{w' \in \mathcal{V}} c(w', w^{i-1}_{i-n+1})} + \frac{\delta N(w^{i-1}_{i-n+1}, \cdot)}{\sum_{w' \in \mathcal{V}} c(w', w^{i-1}_{i-n+1})$$

\subsection{Neural Network Model}

\subsection{Noise Contrastive Estimation}

\section{Experiments}

\section{Conclusion}


\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
